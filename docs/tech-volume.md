## Problem

The method of studying fossils through computed X-ray tomography or synchrotron tomography, despite being touted as "nondestructive," has a high barrier to entry due to the time-consuming and computationally expensive task of manual 3D volume segmentation. These requirements render this powerful technique inaccessible to many paleontologists. This not only exacerbates the existing bottleneck in data processing but also hinders the progress of science as a whole. Despite the wealth of new scientific data that could be gleaned from the relatively small image stacks, the prohibitive costs and technical requirements leave many paleontologists unable to segment their fossils, resulting in missed opportunities for discovery.

## Proposed Solution

Our proposed solution promises to revolutionize the field of paleontology by breaking down the barriers to access and streamlining the data processing pipeline. Our X-ray tomography segmentation pipeline consists of three essential components: an intuitive upload service, an automated segmenter, and a mesh generator. With a user-friendly website interface, paleontologists from anywhere in the world can easily upload their images to the service, where they will be temporarily stored on disk. We will then "chunk" the images into subvolumes, each of which can be processed in parallel. (Filesystem-based chunked volumetric formats such as Zarr make this an efficient and rapid process.) A unique job identifier and token system will enable the researchers to authenticate and monitor the progress of their jobs.

We will then allow the user to upload a small volume of training data, which, combined with our pretrained neural and traditional machine learning models, will enable us to segment and then stitch together the subvolumes. This technique is reliable, and has been used by the big data neuroscience community at large scale for many years.

The final result, the segmented stacks and meshes, will be hosted temporarily for authenticated download. This segmentation solution will empower paleontologists everywhere to unlock the secrets of their fossils and make groundbreaking discoveries at an unprecedented scale.

## Plan for Sharing Outcomes

As part of our commitment to advancing the field of paleontology, we plan to openly share the tools, data, and resources generated by our project with the wider research community. This will include releasing our work through open-source repositories and presenting it at conferences and in peer-reviewed journals. By doing so, we hope to inspire collaboration and further innovation in the field. Additionally, we envision that our online segmentation tool will foster a sense of community among users, who will be encouraged to make their data and segmentations publicly available after publishing their research outcomes. We believe that this not only advances science but also provides an opportunity for users to gain exposure and recognition for their work.

## Potential Future Use

Our X-ray tomography segmentation pipeline has the potential to not only drive individual research forward but also to have a transformative impact on the broader field of paleontology. Its versatility and scalability make it an ideal solution for not just paleontologists but also for researchers across multiple disciplines who work with similar tomographic datasets. The future applications of this pipeline are virtually limitless, and we are confident that it will prove to be a valuable tool for anyone looking to unlock the secrets of their tomographic data. By building a solution that is both flexible and easy to use, we are paving the way for future innovations in data processing and analysis.

## Technical Requirements

-   A web server with a public IP address and static-hosting capability. (Backend support for Python is helpful, but not required.)
-   Compute resources; we anticipate that a single GPU will be sufficient for most use cases by processing serially, though more cards can be used to process in parallel. CPU support is also possible if necessary. This compute resource should be able to expose an HTTP API for the web server to communicate with.
-   Storage space. We anticipate utilization not to exceed 50 GB/mo, with ephemeral bursts to up to 250 GB/mo during data conversion operations. Traffic will be primarily to the CPU/GPU compute resources, with ingress and egress of the complete dataset and byproducts (raw stacks in; segmentations, meshes out) once per job.
-   Database, or mutable, high-speed storage on disk. (All needs can be met by a sqlite database in shared storage.)
